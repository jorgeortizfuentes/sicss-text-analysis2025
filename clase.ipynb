{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on: NLP for Social Science\n",
    "\n",
    "Material preparado para [SICSS Chile 2025](https://sicss.io/2024/chile/). Síentete libre de utilizarlo, modificarlo y compartirlo.\n",
    "\n",
    "**Autor**: Jorge Ortiz Fuentes\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Explorar técnicas de procesamiento de lenguaje natural para obtener insights en ciencias sociales.\n",
    "\n",
    "## Contenidos\n",
    "\n",
    "Los contenidos de la clase son:\n",
    "* Cargar y explorar un corpus.\n",
    "* Preprocesar textos (limpieza, tokenización y lematización).\n",
    "* Explorar textos a nivel de palabras (palabras más frecuentes y n-gramas).\n",
    "* Representar textos en forma de vectores (bag-of-words, TF-IDF, embeddings).\n",
    "* Extraer tópicos de los textos (LDA y BERTopic).\n",
    "* Etiquetar textos con categorías gramáticales (POS tagging) y entidades nombradas (NER).\n",
    "* Clasificar textos usando modelos Transformer preentrenados desde HuggingFace.\n",
    "* Análisis de textos usando IA generativa con Langchain.\n",
    "* Entrenar un clasificador de textos usando embeddings, scikit-learn y xgboost.\n",
    "\n",
    "Para ello, vamos a trabajar con un corpus de 10.000 noticias chilenas del periodo comprendido entre octubre y diciembre de 2019. El corpus se encuentra en el archivo `noticias_oct_dic_2019.tsv` y contiene las siguientes columnas:\n",
    "\n",
    "* `texto`: texto de la noticia.\n",
    "* `canal`: medio de comunicación que publicó la noticia.\n",
    "* `fecha`: fecha de publicación de la noticia.\n",
    "\n",
    "En esta clase trabajaremos con las librerías `pandas`, `spaCy`, `scikit-learn`, `transformers`, `gensim`, `langchain`, `spanish-nlp` y `bertopic` y `sentence-transformers`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga y exploración de textos\n",
    "\n",
    "Las noticias se encuentran en un archivo `tsv` (tab-separated values), que es un formato de archivo de texto plano que utiliza tabuladores para separar los campos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar el archivo de noticias\n",
    "\n",
    "!wget https://raw.githubusercontent.com/jorgeortizfuentes/sicss-text-analysis2025/refs/heads/main/noticias_oct_dic_2019.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"noticias_oct_dic_2019.tsv\", sep=\"\\t\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver cuántas noticias hay por medio en el corpus, podemos usar el método `value_counts()` de `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"medio\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver un texto de ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Texto\")\n",
    "print(df[\"texto\"].iloc[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se desea contar la cantidad de caracteres de un texto se puede ocupar la función `len()` de Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ejemplo = \"soy un texto de prueba de 39 caracteres\"\n",
    "len(ejemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, vamos a contar cuántos caracteres posee cada noticia. Para ello, podemos definir una función que cuente los caracteres de un texto y luego aplicarla a cada noticia usando el método `apply()` de `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar_caracteres(texto):\n",
    "    return len(texto)\n",
    "\n",
    "df[\"caracteres\"] = df[\"texto\"].apply(contar_caracteres)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, podemos obtener el promedio de caracteres por noticia, la desviación estándar, el mínimo y el máximo, y los cuartiles usando el método `describe()` de `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"caracteres\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que la cantidad mínima de caracteres, la máxima y los percentiles 25, 50 y 75. \n",
    "\n",
    "Vemos que el mínimo y el máximo se alejan significativamente de la media de caracteres. \n",
    "\n",
    "Estos datos se consideran `outliers`. Para ver cuáles son los noticias más cortos y más largos, podemos ordenar el `DataFrame` por la columna `caracteres` usando el método `sort_values()` de `pandas`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=\"caracteres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si deseamos ver el texto completo de una fila, podemos usar el método `iloc[]` de `pandas` para acceder a una fila en particular a partir de su índice. Por ejemplo, para ver el texto más largo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1490][\"texto\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a eliminar los `outliers` del corpus. Para ello, vamos a utilizar el Rango Intercuartil (IQR), que corresponde a la diferencia entre el tercer y el primer cuartil. \n",
    "\n",
    "\n",
    "![IQR](https://github.com/jorgeortizfuentes/sicss-text-analysis2025/blob/main/images/iqr.jpg?raw=true)\n",
    "\n",
    "\n",
    "\n",
    "Luego, vamos a eliminar las noticias que se encuentren fuera del rango $[Q_1 - 1.5 \\times IQR]$ (valores menores a este rango) y $[Q_3 + 1.5 \\times IQR]$ (valores mayores a este rango).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el IQR\n",
    "q3 = df[\"caracteres\"].quantile(0.75)\n",
    "q1 = df[\"caracteres\"].quantile(0.25)\n",
    "iqr = q3 - q1\n",
    "\n",
    "iqr1_5_menor = q1 - 1.5 * iqr\n",
    "iqr1_5_mayor = q3 + 1.5 * iqr\n",
    "\n",
    "print(iqr1_5_menor)\n",
    "print(iqr1_5_mayor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos entonces a eliminar los `outliers` del corpus. Esto es particularmente útil cuando se tienen datos con problemas, por ejemplo porque quedaron mal descargados.\n",
    "\n",
    "La cota inferior corresponde a un número negativo, lo que no tiene sentido. Por lo tanto, vamos a definir arbitrariamente el umbral menor como 100 caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_lower_bound = 100\n",
    "\n",
    "df_filtrado = df[(df[\"caracteres\"] > custom_lower_bound) & (df[\"caracteres\"] < iqr1_5_mayor)]\n",
    "\n",
    "# Reseteamos el índice\n",
    "df_filtrado = df_filtrado.reset_index(drop=True)\n",
    "\n",
    "df_filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado[\"caracteres\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de textos\n",
    "\n",
    "Un paquete muy útil para el procesamiento de textos es `spaCy`, que permite realizar diversas tareas de procesamiento de lenguaje natural. Para instalarlo, se puede ejecutar el siguiente comando en la consola:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalación de spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego podemos descargar la versión en español del modelo `spaCy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y podemos cargar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización\n",
    "\n",
    "Para trabajar con los textos, necesitamos separarlos en unidades de análisis. Este proceso se conoce como `tokenización`, pues se separa el texto en `tokens`. En este caso, vamos a separar los textos en palabras o símbolos de puntuación. \n",
    "\n",
    "Seleccionemos un texto al azar para ver cómo funciona la tokenización de `spaCy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_prueba = df_filtrado.iloc[100][\"texto\"]\n",
    "texto_tokenizado = [token.text for token in nlp(texto_prueba)]\n",
    "texto_tokenizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematización\n",
    "\n",
    "Se conoce como `lematización` al proceso de reducir las palabras a su forma base o `lema`. Por ejemplo, el lema de las palabras `corriendo`, `correr` y `corrió` es `correr`.\n",
    "\n",
    "Para lematizar un texto, podemos usar el atributo `lemma_` de cada token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto1 = \"los perros corren\"\n",
    "texto2 = \"el perro corrió\"\n",
    "\n",
    "lemas_texto1 = [token.lemma_ for token in nlp(texto1)]\n",
    "lemas_texto2 = [token.lemma_ for token in nlp(texto2)]\n",
    "\n",
    "print(lemas_texto1)\n",
    "print(lemas_texto2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminación de stopwords\n",
    "\n",
    "Se conocen como `stopwords` a las palabras cuyo significado léxico no es relevante para un análisis a nivel palabra. Por ejemplo, las palabras como las preposiciones, artículos y conjunciones no aportan información relevante para un análisis a nivel palabra.\n",
    "\n",
    "Vamos a utilizar la lista de `stopwords` en español de `spaCy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.lang.es.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_prueba = df_filtrado.iloc[100][\"texto\"]\n",
    "print(texto_prueba)\n",
    "texto_tokenizado = [\n",
    "    token.lemma_ for token in nlp(texto_prueba) if not token.is_stop and not token.is_punct\n",
    "]\n",
    "texto_tokenizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento adicional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muchas veces es necesario realizar un preprocesamiento adicional para eliminar caracteres especiales, URLs, números y para normalizar palabras. Por ejemplo, en los textos encontramos en múltiples ocasiones la palabra  `José` y `Jose`, que corresponden a la misma palabra pero escrita de forma distinta.\n",
    "\n",
    "Para esto podemos ocupar el paquete `spanish_nlp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spanish-nlp\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spanish_nlp import SpanishPreprocess\n",
    "\n",
    "sp = SpanishPreprocess(\n",
    "    lower=True,\n",
    "    remove_url=True,\n",
    "    remove_hashtags=False,\n",
    "    split_hashtags=True,\n",
    "    normalize_breaklines=True,\n",
    "    remove_emoticons=False,\n",
    "    remove_emojis=False,\n",
    "    convert_emoticons=False,\n",
    "    convert_emojis=False,\n",
    "    normalize_inclusive_language=True,\n",
    "    reduce_spam=True,\n",
    "    remove_vowels_accents=True,\n",
    "    remove_multiple_spaces=True,\n",
    "    remove_punctuation=True,\n",
    "    remove_unprintable=True,\n",
    "    remove_numbers=True,\n",
    "    remove_stopwords=False,\n",
    "    stopwords_list=None,\n",
    "    lemmatize=False,\n",
    "    stem=False,\n",
    "    remove_html_tags=True,\n",
    ")\n",
    "\n",
    "test_text = \"\"\"𝓣𝓮𝔁𝓽𝓸 𝓭𝓮 𝓹𝓻𝓾𝓮𝓫𝓪\n",
    "\n",
    "<b>Holaaaaaaaa a todxs </b>, este es un texto de prueba :) a continuación les mostraré un poema de Roberto Bolaño llamado \"Los perros románticos\" 🤭👀😅\n",
    "\n",
    "https://www.poesi.as/rb9301.htm\n",
    "\n",
    "¡Me gustan los pingüinos! Sí, los PINGÜINOS 🐧🐧🐧 🐧 #VivanLosPinguinos #SíSeñor #PinguinosDelMundoUníos #ÑanduesDelMundoTambién\n",
    "\n",
    "Si colaboras con este repositorio te puedes ganar $100.000 (en dinero falso). O tal vez 20 pingüinos. Mi teléfono es +561212121212\"\"\"\n",
    "\n",
    "print(sp.transform(test_text, debug=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a aplicar este preprocesamiento a los textos y guardarlos en una nueva columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado[\"texto_pp\"] = df_filtrado[\"texto\"].apply(lambda x: sp.transform(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración a nivel de palabras\n",
    "\n",
    "Ahora, vamos a explorar el corpus a nivel de palabras. Para ello, vamos a crear una función que reciba un texto y devuelva una lista con las palabras tokenizadas, en minúsculas, lematizadas, sin `stopwords` y sin números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizar(text):\n",
    "    doc = nlp(text)\n",
    "    return [\n",
    "        token.lemma_ for token in doc if not token.is_punct and not token.is_stop and token.is_alpha\n",
    "    ]\n",
    "\n",
    "\n",
    "df_filtrado[\"tokens\"] = df_filtrado[\"texto_pp\"].apply(tokenizar)\n",
    "df_filtrado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar la función `Counter` de la librería `collections` para contar la frecuencia de las palabras en el corpus. Luego, vamos a mostrar las 30 palabras más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Unir todos los tokens en una lista\n",
    "all_tokens = [token for tokens in df_filtrado[\"tokens\"] for token in tokens]\n",
    "\n",
    "# Contar la frecuencia de las palabras\n",
    "word_freq = Counter(all_tokens)\n",
    "\n",
    "# Obtener las 20 palabras más frecuentes\n",
    "top_words = word_freq.most_common(30)\n",
    "\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gramas más comunes\n",
    "\n",
    "Los n-gramas son secuencias de n palabras. Por ejemplo, los bi-gramas de la oración `El perro juega con su pelota` son:\n",
    "- `El perro`\n",
    "- `perro juega`\n",
    "- `juega con`\n",
    "- `con su`\n",
    "- `su pelota`\n",
    "\n",
    "Los n-gramas son útiles para detectar frases o expresiones que se repiten en el corpus. Para ello, vamos a utilizar `CountVectorizer` de `scikit-learn`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "stopwords = list(spacy.lang.es.stop_words.STOP_WORDS)\n",
    "\n",
    "# Crear instancia de CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, vamos a utilizar el método `fit_transform` de la instancia de `CountVectorizer` para crear una matriz de conteo de n-gramas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear matriz de conteo de n-gramas\n",
    "ngram_matrix = vectorizer.fit_transform(df_filtrado[\"texto_pp\"])\n",
    "\n",
    "ngram_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, vamos a sumar las frecuencias de los n-gramas y a mostrar los n-gramas más comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sumar las frecuencias de los n-gramas\n",
    "ngram_freq = np.sum(ngram_matrix, axis=0)\n",
    "\n",
    "# Obtener los n-gramas más comunes\n",
    "top_ngrams = [(ngram, ngram_freq[0, index]) for ngram, index in vectorizer.vocabulary_.items()]\n",
    "top_ngrams = sorted(top_ngrams, key=lambda x: x[1], reverse=True)[:30]\n",
    "\n",
    "print(top_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculemos ahora las palabras y los bigramas más frecuentes por medio. Para ello concatenaremos todos los textos de cada medio y luego calcularemos sus n_gramas más repetidas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medios = df_filtrado[\"medio\"].unique().tolist()\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words=stopwords)\n",
    "\n",
    "# Iterar sobre los medios\n",
    "for medio in medios:\n",
    "    # Filtrar los textos por medio\n",
    "    textos_year = df_filtrado[df_filtrado[\"medio\"] == medio][\"texto_pp\"]\n",
    "\n",
    "    # Unir todos los textos en un solo string\n",
    "    texto_completo = \" \".join(textos_year)\n",
    "\n",
    "    # Crear la matriz de n-gramas\n",
    "    ngram_matrix_year = vectorizer.fit_transform([texto_completo])\n",
    "\n",
    "    # Sumar las frecuencias de los n-gramas\n",
    "    ngram_freq_year = np.sum(ngram_matrix_year, axis=0)\n",
    "\n",
    "    # Obtener los n-gramas y sus frecuencias\n",
    "    top_10_ngrams_year = [\n",
    "        (ngram, ngram_freq_year[0, index]) for ngram, index in vectorizer.vocabulary_.items()\n",
    "    ]\n",
    "\n",
    "    # Ordenar por frecuencia y tomar los 10 más frecuentes\n",
    "    top_10_ngrams_year = sorted(top_10_ngrams_year, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    # Imprimir los resultados para el medio actual\n",
    "    print(f\"Medio: {medio}\")\n",
    "    print(top_10_ngrams_year)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorización de textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder hacer análisis más profundos de los textos, necesitamos representarlos en forma numérica. Este proceso se conoce como `vectorización`, es decir, transformar los textos en vectores (secuencias de números).\n",
    "\n",
    "Existen diversas formas de vectorizar textos. En esta clase vamos a ver dos de ellas: `bag-of-words` y `TF-IDF`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "\n",
    "Se conoce como `bag-of-words` a la representación de un texto como un vector que contiene la frecuencia de cada palabra en el texto. Es decir, se crea un vector con todas las palabras del corpus y se cuenta cuántas veces aparece cada palabra en cada texto.\n",
    "\n",
    "![Bag of ords](https://vitalflux.com/wp-content/uploads/2021/08/Bag-of-words-technique-to-convert-to-numerical-feature-vector-png.png)\n",
    "\n",
    "También, podemos contar la frecuencia de los n-gramas en cada texto. Para ello, nuevamente, vamos a utilizar `CountVectorizer` de `scikit-learn`. Esta vez con el parámetro `ngram_range` (1, 1) (por defecto) para contar palabras y con el parámetro `ngram_range` (1, 2) para contar palabras y 2-gramas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_by_word = CountVectorizer(ngram_range=(1, 1), stop_words=stopwords)\n",
    "\n",
    "# Entrenamos el vectorizador\n",
    "vectorizer_by_word.fit(df_filtrado[\"texto_pp\"])\n",
    "\n",
    "# Lo aplicamos al texto de ejemplo\n",
    "vectorizer_by_word.transform([texto_prueba])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado del texto es un vector muy  largo, que corresponde a la cantidad de palabras únicas en el corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a vectorizar un texto con `ngram_range` (1, 2) para contar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_by_2gram = CountVectorizer(ngram_range=(1, 2), stop_words=stopwords)\n",
    "\n",
    "# Entrenamos el vectorizador\n",
    "vectorizer_by_2gram.fit(df_filtrado[\"texto_pp\"])\n",
    "\n",
    "# Lo aplicamos al texto de ejemplo\n",
    "vectorizer_by_2gram.transform([texto_prueba])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos una matriz aún más grande que la anterior. Esto se debe a que ahora estamos contando palabras y 2-gramas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una dimensionalidad muy alta puede ser un problema, debido a que es más costoso computacionalmente de procesar. Por lo tanto, vamos a limitar la cantidad de palabras que se van a considerar en la vectorización. Para ello, vamos a utilizar el parámetro `max_features` de `CountVectorizer` para limitar la cantidad de palabras y n-gramas a los 1000 más frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_by_2gram = CountVectorizer(ngram_range=(1, 2), max_features=1000, stop_words=stopwords)\n",
    "\n",
    "# Entrenamos el vectorizador\n",
    "vectorizer_by_2gram.fit(df_filtrado[\"texto_pp\"])\n",
    "\n",
    "# Lo aplicamos al texto de ejemplo\n",
    "vectorizer_by_2gram.transform([texto_prueba])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora aplicamos la vectorización a todos los textos del corpus y creamos una matriz de 1000 columnas y tantas filas como textos haya en el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_vectorizado_bow = vectorizer_by_2gram.transform(df_filtrado[\"texto_pp\"])\n",
    "\n",
    "# Lo convertimos a un DataFrame con los nombres de las columnas\n",
    "df_corpus_vectorizado_bow = pd.DataFrame(\n",
    "    corpus_vectorizado_bow.toarray(), columns=vectorizer_by_2gram.get_feature_names_out()\n",
    ")\n",
    "df_corpus_vectorizado_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque puede parecer una vectorización sencilla y poco sofisticada, `bag-of-words` es una técnica bastante útil ya que logra capturar el significado de los textos. Por ejemplo, si tenemos dos textos que hablan de fútbol, es probable que compartan palabras como `gol`, `equipo`, `jugador`, `partido`, etc. Por lo tanto, es probable que los vectores de estos textos sean similares. Por ejemplo, podemos buscar el texto que más se parece a `texto_prueba`. \n",
    "\n",
    "Para ello recorremos todos los textos del corpus y calculamos la similitud `coseno` con el texto de prueba. La similitud coseno es una medida de similitud entre dos vectores que mide el coseno del ángulo entre ellos. Si los vectores son idénticos, la similitud coseno es 1. Si los vectores son ortogonales, la similitud coseno es 0. Si los vectores son opuestos, la similitud coseno es -1.\n",
    "\n",
    "Para calcular la similitud coseno, vamos a utilizar la función `cosine_similarity` de `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Hay que calcular la similitud entre el texto de prueba y todos los demás.\n",
    "matriz_similitud = cosine_similarity(corpus_vectorizado_bow[100], corpus_vectorizado_bow)\n",
    "\n",
    "# Ordenar los textos por similitud\n",
    "textos_similares = list(zip(df_filtrado[\"texto_pp\"], matriz_similitud[0]))\n",
    "textos_similares.sort(key=lambda x: -x[1])\n",
    "\n",
    "# Mostrar los 5 textos más similares\n",
    "for texto, similitud in textos_similares[:6]:\n",
    "    print(f\"Similitud: {similitud}\\n{texto}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Aunque `bag-of-words` puede ser útil, no logra capturar la importancia de las palabras, ya que solo cuenta su frecuencia. Para ello, se puede usar `TF-IDF` (Term Frequency - Inverse Doucument Frequency), que cuenta la frecuencia de las palabras ponderada por su frecuencia inversa en los textos. Es decir, si una palabra aparece en muchos textos, no aporta mucha información para distinguir entre textos, por lo tanto se le asigna una ponderación menor.\n",
    "\n",
    "Para aplicar `TF-IDF` sobre el corpus, vamos a utilizar la función `TfidfVectorizer` de `scikit-learn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Creamos la instancia de TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=1000, stop_words=stopwords)\n",
    "\n",
    "# Entrenamos el vectorizador y transformamos el texto\n",
    "# (lo hacemos en un solo paso con fit_transform)\n",
    "corpus_vectorizado_tfidf = vectorizer.fit_transform(df_filtrado[\"texto_pp\"])\n",
    "\n",
    "# convertir a DataFrame\n",
    "df_corpus_vectorizado_tfidf = pd.DataFrame(\n",
    "    corpus_vectorizado_tfidf.toarray(), columns=vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# Calculamos la similitud del texto de prueba con los demás\n",
    "matriz_similitud_tfidf = cosine_similarity(corpus_vectorizado_tfidf[100], corpus_vectorizado_tfidf)\n",
    "\n",
    "# Ordenamos los textos por similitud\n",
    "textos_similares_tfidf = list(zip(df_filtrado[\"texto_pp\"], matriz_similitud_tfidf[0]))\n",
    "textos_similares_tfidf.sort(key=lambda x: -x[1])\n",
    "\n",
    "# Mostramos los 5 textos más similares\n",
    "for texto, similitud in textos_similares_tfidf[:5]:\n",
    "    print(f\"Similitud: {similitud}\\n{texto}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraer tópicos de los textos\n",
    "\n",
    "Cuando tenemos muchos textos, es útil poder extraer los tópicos de los textos, es decir, las temáticas que abordan los textos. Para ello, vamos a utilizar dos métodos.\n",
    "\n",
    "Para ello ocuparemos `Latent Dirichlet Allocation` (LDA), que es un modelo generativo de tópicos. Este modelo asume que cada texto se compone de una mezcla de tópicos y que cada tópico se compone de una mezcla de palabras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA con TF-IDF\n",
    "\n",
    "Vamos a aplicar LDA a la matriz generada por `tf-idf`. Si bien también podríamos ocupar Bag of Words, TF-IDF es más adecuado para LDA, ya que pondera las palabras por su importancia en los textos.\n",
    "\n",
    "Creamos la instancia de LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=0)\n",
    "\n",
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos LDA\n",
    "lda.fit(corpus_vectorizado_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos obtener los tópicos con el método `components_` de la instancia de LDA:\n",
    "\n",
    "topicos = lda.components_\n",
    "topicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos una matriz de 10 tópicos y 1000 palabras.\n",
    "\n",
    "Vamos a mostrar las 20 palabras que componen cada tópico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las palabras de cada tópico\n",
    "words = vectorizer_by_2gram.get_feature_names_out()\n",
    "\n",
    "top_words_per_topic = []\n",
    "\n",
    "for numero_topico in range(10):\n",
    "    print(f\"\\nTópico {numero_topico + 1}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Obtener los pesos de las palabras para este tópico\n",
    "    pesos_palabras = topicos[numero_topico]\n",
    "\n",
    "    # Crear lista de tuplas (palabra, peso)\n",
    "    palabras_y_pesos = list(zip(words, pesos_palabras))\n",
    "\n",
    "    # Ordenar por peso de mayor a menor\n",
    "    palabras_y_pesos = sorted(palabras_y_pesos, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Mostrar las 20 primeras palabras\n",
    "    for palabra, peso in palabras_y_pesos[:10]:\n",
    "        print(f\"Palabra: {palabra:<20} Peso: {peso:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de Tópicos con BERTopic\n",
    "\n",
    "LDA (Latent Dirichlet Allocation) es un método popular para la extracción de tópicos, pero presenta algunas limitaciones:\n",
    "\n",
    "- LDA supone que los documentos se generan a partir de una mezcla de tópicos y que cada tópico es una mezcla de palabras, lo cual puede no ser cierto en todos los casos.\n",
    "- LDA no siempre maneja bien documentos cortos o con tópicos poco definidos.\n",
    "\n",
    "### BERTopic: un enfoque más moderno\n",
    "\n",
    "BERTopic utiliza modelos de lenguaje basados en `Transformers` (como BERT) para generar representaciones vectoriales de textos (`embeddings`). Luego, agrupa estos vectores para identificar tópicos, lo que ofrece ventajas sobre LDA:\n",
    "- No asume una distribución específica de palabras en los tópicos.\n",
    "- Capta mejor el contexto semántico de los textos.\n",
    "- Es más robusto con textos cortos\n",
    "\n",
    "\n",
    "### Instalación de BERTopic\n",
    "\n",
    "Para comenzar, instalamos la librería con el siguiente comando:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bertopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos la librería y creamos una instancia de BERTopic. Utilizaremos el modelo multilingüe `paraphrase-multilingual-MiniLM-L12-v2`, que soporta español:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# Crear una instancia del modelo BERTopic\n",
    "topic_model = BERTopic(\n",
    "    language=\"multilingual\", embedding_model=\"paraphrase-multilingual-MiniLM-L12-v2\", verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo con los textos originales. No es necesario preprocesar los textos, ya que los modelos de lenguaje basados en Transformers manejan bien los textos en bruto.\n",
    "\n",
    "El resultado será un DataFrame que incluye:\n",
    "- ID del tópico: Identificador único.\n",
    "- Frecuencia: Número de textos asignados al tópico.\n",
    "- Palabras representativas: Términos más importantes del tópico.\n",
    "- Tópico -1: Corresponde a los textos que no pudieron asignarse a ningún tópico (outliers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo y obtener los tópicos\n",
    "topics, probs = topic_model.fit_transform(df_filtrado[\"texto\"])\n",
    "\n",
    "# Obtener información detallada de los tópicos\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignar tópicos a los textos\n",
    "df_filtrado[\"topico_bertopic\"] = topics\n",
    "df_filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el tópico más frecuente por medio\n",
    "\n",
    "\n",
    "for medio in sorted(df_filtrado[\"medio\"].unique()):\n",
    "    # Filter comments for this year\n",
    "    noticias_medio = df_filtrado[df_filtrado[\"medio\"] == medio]\n",
    "\n",
    "    # Filter comments with no topic (-1)\n",
    "    noticias_medio = noticias_medio[noticias_medio[\"topico_bertopic\"] != -1]\n",
    "\n",
    "    # Count frequency of each topic\n",
    "    frecuencia_topicos = noticias_medio[\"topico_bertopic\"].value_counts()\n",
    "\n",
    "    # Get most frequent topic\n",
    "    topico_mas_frecuente = frecuencia_topicos.idxmax()\n",
    "\n",
    "    # Get topic info for the most frequent topic\n",
    "    info_topico = topic_model.get_topic(topico_mas_frecuente)\n",
    "\n",
    "    print(f\"\\Medio: {medio}\")\n",
    "    print(f\"Tópico más frecuente: {topico_mas_frecuente}\")\n",
    "    print(f\"Frecuencia: {frecuencia_topicos[topico_mas_frecuente]}\")\n",
    "    print(\"Palabras más representativas:\")\n",
    "    for palabra, peso in info_topico[:10]:\n",
    "        print(f\"- {palabra}: {peso:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis lingüísticos\n",
    "\n",
    "A continuación vamos a etiquetar textos mediante análisis lingüísticos. En concreto, vamos a realizar las siguientes tareas:\n",
    "\n",
    "* Análisis de categorías gramáticales (POS tagging)\n",
    "* Análisis de entidades nombradas (NER)\n",
    "\n",
    "En concreto, vamos a utilizar Spacy para realizar el etiquetado de tokens.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para etiquetar un texto con categorías gramáticas, podemos usar el atributo `pos_` de cada token. Este atributo etiqueta las palabras con categorías universales, como Sustantivo, Verbo, Adjetivo, etc.\n",
    "texto_prueba = df_filtrado.iloc[13][\"texto\"]\n",
    "print(texto_prueba)\n",
    "pos_texto = [(token.text, token.pos_) for token in nlp(texto_prueba)]\n",
    "pos_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para identificar las entidades nombradas, podemos usar el atributo `ents` del resultado de procesar un texto con `nlp()`. Este atributo retorna una lista de entidades, cada una con su tipo (como Persona, Lugar, Organización, etc) y el texto de la entidad.\n",
    "\n",
    "texto_prueba = df_filtrado.iloc[20][\"texto\"]\n",
    "print(texto_prueba)\n",
    "entidades = [(ent.text, ent.label_) for ent in nlp(texto_prueba).ents]\n",
    "entidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de textos\n",
    "\n",
    "En muchas ocasiones, es necesario clasificar textos en distintas categorías. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos preentrenados con HuggingFace\n",
    "\n",
    "HuggingFace es una librería que permite acceder a modelos preentrenados de lenguaje natural. En particular, vamos a utilizar modelos preentrenados para clasificar textos en las siguientes categorías:\n",
    "\n",
    "- Análisis de sentimientos\n",
    "- Hate speech\n",
    "\n",
    "Puedes encontrar más información sobre los modelos preentrenados en español de HuggingFace en [este enlace](https://huggingface.co/models?pipeline_tag=text-classification&language=es&sort=trending)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero vamos a instalar la librería `transformers` de HuggingFace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a utilizar el modelo `pysentimiento/robertuito-sentiment-analysis` para clasificar los textos dependiendo de su sentimiento. Este modelo clasifica los textos en tres categorías: `positivo`, `negativo` y `neutro`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Crea una instancia del pipeline de análisis de sentimiento\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"pysentimiento/robertuito-sentiment-analysis\",\n",
    "    return_all_scores=True,\n",
    ")\n",
    "\n",
    "# Texto de prueba\n",
    "texto_prueba = \"la constitución es un mamarracho\"\n",
    "\n",
    "sentiment_analyzer(texto_prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a utilizar el modelo `pysentimiento/robertuito-hate-speech` para clasificar los textos dependiendo de si contienen discurso de odio o no. Este modelo clasifica los textos en las siguientes clases:\n",
    "- HS: si el texto contiene discurso de odio.\n",
    "- TR: si el texto está dirigido a una persona específica.\n",
    "- AG: si el texto es agresivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una instancia del pipeline de análisis de hate\n",
    "hate_analyzer = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"pysentimiento/robertuito-hate-speech\",\n",
    "    return_all_scores=True,\n",
    ")\n",
    "\n",
    "# Texto de prueba\n",
    "texto_prueba = \"los constituyentes son unos estupidos\"\n",
    "\n",
    "hate_analyzer(texto_prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procesemos ahora todo el dataset con el modelo de análisis de sentimientos y de hate speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filtrado[\"sentimiento\"] = df_filtrado[\"texto\"].apply(lambda x: sentiment_analyzer(x))\n",
    "# df_filtrado[\"hate_speech\"] = df_filtrado[\"texto\"].apply(lambda x: hate_analyzer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de textos con IA generativa\n",
    "\n",
    "La IA generativa ha ganado popularidad en los últimos años para el análisis de textos ya que no requiere de datos preetiquetados. Basta con introducir las instrucciones necesarias para que el modelo genere clasificaciones, análisis, resúmenes, traducciones o lo que se necesite.\n",
    "\n",
    "Existen múltiples modelos de IA generativa, como ChatGPT, Gemini, Llama, Claude, entre otros. \n",
    "\n",
    "En esta clase vamos a utilizar Langchain, un paquete que nos permite trabajar con IA generativa de forma agnóstica a los modelos. \n",
    "\n",
    "A continuación, ocuparemos Gemini, la IA generativa de Google, para detectar las noticias que contienen sesgos. \n",
    "\n",
    "Primero, vamos a instalar las dependencias necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ocupar los modelos de IA generativa necesitas una API Key del proveedor. En este caso, usaremos la API de ChatGPT.\n",
    "\n",
    "Para obtener la API Key, debes registrarte en OpenAI, ingresar tu tarjeta de crédito y generar la API Key.\n",
    "    \n",
    "https://platform.openai.com/docs/overview\n",
    "\n",
    "Considera que el uso de estos servicios tiene un costo asociado. Puedes ver el detalle de los precios en la siguiente página:\n",
    "    \n",
    "https://openai.com/api/pricing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = (\n",
    "    \"xxx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,  # Grado de creatividad. 0 significa que es poco creativo y 1 es muy creativo\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "llm.invoke(\"Saludame en mapudungun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "tagging_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Analiza la siguiente noticia y determina si el contenido o la forma de estar escrito puede generar temor en el lector.\n",
    "\n",
    "Entenderemos como temor la sensación de miedo, angustia o preocupación que puede generar un texto en el lector.\n",
    "\n",
    "Solo extrae las propiedades mencionadas en la función 'Classification'.\n",
    "\n",
    "Texto:\n",
    "{texto}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class Classification(BaseModel):\n",
    "    razonamiento: str = Field(description=\"El razonamiento detrás de la clasificación\")\n",
    "    temor: bool = Field(description=\"Si el texto genera o no temor\")\n",
    "\n",
    "\n",
    "chain = tagging_prompt | llm.with_structured_output(Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticia = df[\"texto\"].iloc[2]\n",
    "\n",
    "resultado = chain.invoke({\"texto\": noticia})\n",
    "print(\"Noticia:\")\n",
    "print(noticia)\n",
    "print(\"\\n\\nRazonamiento:\")\n",
    "print(resultado.razonamiento)\n",
    "print(\"\\n\\nGenera temor:\", resultado.temor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_filtrado_sample = df_filtrado.sample(200, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_chain(texto):\n",
    "    resultado = chain.invoke({\"texto\": texto})\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Crea 3 nuevas columnas en el DataFrame con los resultados de la clasificación\n",
    "# df_filtrado[\"clasificacion_estereotipo\"] = df_filtrado[\"texto\"].apply(aplicar_chain)\n",
    "\n",
    "# df_filtrado[\"razonamiento\"] = df_filtrado[\"clasificacion_estereotipo\"].apply(\n",
    "#     lambda x: x.razonamiento\n",
    "# )\n",
    "# df_filtrado[\"genera_temor\"] = df_filtrado[\"clasificacion_estereotipo\"].apply(lambda x: x.temor)\n",
    "# df_filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filtrado.to_csv(\"noticias_procesadas.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para no esperar tanto tiempo, descargamos el archivo ya procesado\n",
    "!wget https://github.com/jorgeortizfuentes/sicss-text-analysis2025/raw/refs/heads/main/noticias_procesadas.tsv\n",
    "\n",
    "df_filtrado = pd.read_csv(\"noticias_procesadas.tsv\", sep=\"\\t\")\n",
    "\n",
    "df_filtrado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando un clasificador\n",
    "\n",
    "El uso de IA generativa es muy útil para el análisis de textos. Sin embargo, es caro y no genera resultados consistentes. \n",
    "\n",
    "El enfoque clásico para entrenar un clasificador de texto requiere de datos etiquetados. Por lo tanto, podemos aprovechar la IA generativa para generar datos etiquetados y luego entrenar un clasificador de texto.\n",
    "\n",
    "En esta ocasión utilizaremos los embeddings de ´paraphrase-multilingual-MiniLM-L12-v2 para convertir los textos en vectores y luego entrenaremos un modelo utilizando scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentences = [\"Esto es un texto de ejemplo\"]\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a leer el archivo guardado\n",
    "df_filtrado = pd.read_csv(\"noticias_procesadas.tsv\", sep=\"\\t\")\n",
    "df_filtrado[\"genera_temor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizar el df_filtrado con el modelo SentenceTransformer\n",
    "\n",
    "df_filtrado[\"embeddings\"] = df_filtrado[\"texto\"].apply(lambda x: model.encode([x])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo de random forest para clasificar los textos (con genera_temor y embeddings)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rfc_model = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Dividir dataset\n",
    "X = df_filtrado[\"embeddings\"].tolist()\n",
    "y = df_filtrado[\"genera_temor\"].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Entrenar modelo\n",
    "rfc_model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir\n",
    "y_pred = rfc_model.predict(X_test)\n",
    "\n",
    "# Mostrar reporte de clasificación\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos ahora con xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=0)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Material complementario \n",
    "\n",
    "* [Speech and Language Processing (Jurafsky and Martin)](https://web.stanford.edu/~jurafsky/slp3/)\n",
    "* [spaCy 101](https://spacy.io/usage/spacy-101)\n",
    "* [LangChain Tutorials](https://python.langchain.com/docs/tutorials/)\n",
    "* [Spanish NLP library](https://github.com/jorgeortizfuentes/spanish_nlp) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
