{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on: NLP for Social Science\n",
    "\n",
    "Material preparado para [SICSS Chile 2025](https://sicss.io/2024/chile/). S칤entete libre de utilizarlo, modificarlo y compartirlo.\n",
    "\n",
    "**Autor**: Jorge Ortiz Fuentes\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Explorar t칠cnicas de procesamiento de lenguaje natural para obtener insights en ciencias sociales.\n",
    "\n",
    "## Contenidos\n",
    "\n",
    "Los contenidos de la clase son:\n",
    "* Cargar y explorar un corpus.\n",
    "* Preprocesar textos (limpieza, tokenizaci칩n y lematizaci칩n).\n",
    "* Explorar textos a nivel de palabras (palabras m치s frecuentes y n-gramas).\n",
    "* Representar textos en forma de vectores (bag-of-words, TF-IDF, embeddings).\n",
    "* Extraer t칩picos de los textos (LDA y BERTopic).\n",
    "* Etiquetar textos con categor칤as gram치ticales (POS tagging) y entidades nombradas (NER).\n",
    "* Clasificar textos usando modelos Transformer preentrenados desde HuggingFace.\n",
    "* An치lisis de textos usando IA generativa con Langchain.\n",
    "* Entrenar un clasificador de textos usando embeddings, scikit-learn y xgboost.\n",
    "\n",
    "Para ello, vamos a trabajar con un corpus de 10.000 noticias chilenas del periodo comprendido entre octubre y diciembre de 2019. El corpus se encuentra en el archivo `noticias_oct_dic_2019.tsv` y contiene las siguientes columnas:\n",
    "\n",
    "* `texto`: texto de la noticia.\n",
    "* `canal`: medio de comunicaci칩n que public칩 la noticia.\n",
    "* `fecha`: fecha de publicaci칩n de la noticia.\n",
    "\n",
    "En esta clase trabajaremos con las librer칤as `pandas`, `spaCy`, `scikit-learn`, `transformers`, `gensim`, `langchain`, `spanish-nlp` y `bertopic` y `sentence-transformers`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga y exploraci칩n de textos\n",
    "\n",
    "Las noticias se encuentran en un archivo `tsv` (tab-separated values), que es un formato de archivo de texto plano que utiliza tabuladores para separar los campos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar el archivo de noticias\n",
    "\n",
    "!wget https://raw.githubusercontent.com/jorgeortizfuentes/sicss-text-analysis2025/refs/heads/main/noticias_oct_dic_2019.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"noticias_oct_dic_2019.tsv\", sep=\"\\t\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver cu치ntas noticias hay por medio en el corpus, podemos usar el m칠todo `value_counts()` de `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"medio\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver un texto de ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Texto\")\n",
    "print(df[\"texto\"].iloc[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se desea contar la cantidad de caracteres de un texto se puede ocupar la funci칩n `len()` de Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ejemplo = \"soy un texto de prueba de 39 caracteres\"\n",
    "len(ejemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci칩n, vamos a contar cu치ntos caracteres posee cada noticia. Para ello, podemos definir una funci칩n que cuente los caracteres de un texto y luego aplicarla a cada noticia usando el m칠todo `apply()` de `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar_caracteres(texto):\n",
    "    return len(texto)\n",
    "\n",
    "df[\"caracteres\"] = df[\"texto\"].apply(contar_caracteres)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, podemos obtener el promedio de caracteres por noticia, la desviaci칩n est치ndar, el m칤nimo y el m치ximo, y los cuartiles usando el m칠todo `describe()` de `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"caracteres\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que la cantidad m칤nima de caracteres, la m치xima y los percentiles 25, 50 y 75. \n",
    "\n",
    "Vemos que el m칤nimo y el m치ximo se alejan significativamente de la media de caracteres. \n",
    "\n",
    "Estos datos se consideran`outliers`. Para ver cu치les son los noticias m치s cortos y m치s largos, podemos ordenar el `DataFrame` por la columna `caracteres` usando el m칠todo `sort_values()` de `pandas`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=\"caracteres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si deseamos ver el texto completo de una fila, podemos usar el m칠todo `iloc[]` de `pandas` para acceder a una fila en particular a partir de su 칤ndice. Por ejemplo, para ver el texto m치s largo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1490][\"texto\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a eliminar los `outliers` del corpus. Para ello, vamos a utilizar el Rango Intercuartil (IQR), que corresponde a la diferencia entre el tercer y el primer cuartil. \n",
    "\n",
    "\n",
    "![IQR](https://github.com/jorgeortizfuentes/sicss-text-analysis2025/blob/main/images/iqr.jpg?raw=true)\n",
    "\n",
    "\n",
    "\n",
    "Luego, vamos a eliminar las noticias que se encuentren fuera del rango $[Q_1 - 1.5 \\times IQR]$ (valores menores a este rango) y $[Q_3 + 1.5 \\times IQR]$ (valores mayores a este rango).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el IQR\n",
    "q3 = df[\"caracteres\"].quantile(0.75)\n",
    "q1 = df[\"caracteres\"].quantile(0.25)\n",
    "iqr = q3 - q1\n",
    "\n",
    "iqr1_5_menor = q1 - 1.5 * iqr\n",
    "iqr1_5_mayor = q3 + 1.5 * iqr\n",
    "\n",
    "print(iqr1_5_menor)\n",
    "print(iqr1_5_mayor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos entonces a eliminar los `outliers` del corpus. Esto es particularmente 칰til cuando se tienen datos con problemas, por ejemplo porque quedaron mal descargados.\n",
    "\n",
    "La cota inferior corresponde a un n칰mero negativo, lo que no tiene sentido. Por lo tanto, vamos a definir arbitrariamente el umbral menor como 100 caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_lower_bound = 100\n",
    "\n",
    "df_filtrado = df[(df[\"caracteres\"] > custom_lower_bound) & (df[\"caracteres\"] < iqr1_5_mayor)]\n",
    "\n",
    "# Reseteamos el 칤ndice\n",
    "df_filtrado = df_filtrado.reset_index(drop=True)\n",
    "\n",
    "df_filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado[\"caracteres\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de textos\n",
    "\n",
    "Un paquete muy 칰til para el procesamiento de textos es `spaCy`, que permite realizar diversas tareas de procesamiento de lenguaje natural. Para instalarlo, se puede ejecutar el siguiente comando en la consola:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalaci칩n de spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego podemos descargar la versi칩n en espa침ol del modelo `spaCy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y podemos cargar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizaci칩n\n",
    "\n",
    "Para trabajar con los textos, necesitamos separarlos en unidades de an치lisis. Este proceso se conoce como `tokenizaci칩n`, pues se separa el texto en `tokens`. En este caso, vamos a separar los textos en palabras o s칤mbolos de puntuaci칩n. \n",
    "\n",
    "Seleccionemos un texto al azar para ver c칩mo funciona la tokenizaci칩n de `spaCy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_prueba = df_filtrado.iloc[100][\"texto\"]\n",
    "texto_tokenizado = [token.text for token in nlp(texto_prueba)]\n",
    "texto_tokenizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematizaci칩n\n",
    "\n",
    "Se conoce como `lematizaci칩n` al proceso de reducir las palabras a su forma base o `lema`. Por ejemplo, el lema de las palabras `corriendo`, `correr` y `corri칩` es `correr`.\n",
    "\n",
    "Para lematizar un texto, podemos usar el atributo `lemma_` de cada token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto1 = \"los perros corren\"\n",
    "texto2 = \"el perro corri칩\"\n",
    "\n",
    "lemas_texto1 = [token.lemma_ for token in nlp(texto1)]\n",
    "lemas_texto2 = [token.lemma_ for token in nlp(texto2)]\n",
    "\n",
    "print(lemas_texto1)\n",
    "print(lemas_texto2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminaci칩n de stopwords\n",
    "\n",
    "Se conocen como `stopwords` a las palabras cuyo significado l칠xico no es relevante para un an치lisis a nivel palabra. Por ejemplo, las palabras como las preposiciones, art칤culos y conjunciones no aportan informaci칩n relevante para un an치lisis a nivel palabra.\n",
    "\n",
    "Vamos a utilizar la lista de `stopwords` en espa침ol de `spaCy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.lang.es.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_prueba = df_filtrado.iloc[100][\"texto\"]\n",
    "print(texto_prueba)\n",
    "texto_tokenizado = [\n",
    "    token.lemma_ for token in nlp(texto_prueba) if not token.is_stop and not token.is_punct\n",
    "]\n",
    "texto_tokenizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento adicional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muchas veces es necesario realizar un preprocesamiento adicional para eliminar caracteres especiales, URLs, n칰meros y para normalizar palabras. Por ejemplo, en los textos encontramos en m칰ltiples ocasiones la palabra  `Jos칠` y `Jose`, que corresponden a la misma palabra pero escrita de forma distinta.\n",
    "\n",
    "Para esto podemos ocupar el paquete `spanish_nlp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spanish-nlp\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spanish_nlp import SpanishPreprocess\n",
    "\n",
    "sp = SpanishPreprocess(\n",
    "    lower=True,\n",
    "    remove_url=True,\n",
    "    remove_hashtags=False,\n",
    "    split_hashtags=True,\n",
    "    normalize_breaklines=True,\n",
    "    remove_emoticons=False,\n",
    "    remove_emojis=False,\n",
    "    convert_emoticons=False,\n",
    "    convert_emojis=False,\n",
    "    normalize_inclusive_language=True,\n",
    "    reduce_spam=True,\n",
    "    remove_vowels_accents=True,\n",
    "    remove_multiple_spaces=True,\n",
    "    remove_punctuation=True,\n",
    "    remove_unprintable=True,\n",
    "    remove_numbers=True,\n",
    "    remove_stopwords=False,\n",
    "    stopwords_list=None,\n",
    "    lemmatize=False,\n",
    "    stem=False,\n",
    "    remove_html_tags=True,\n",
    ")\n",
    "\n",
    "test_text = \"\"\"洧닊洧닕洧대洧닣洧닞 洧닔洧닕 洧닟洧닡洧쮫롑쉻롑洧닑\n",
    "\n",
    "<b>Holaaaaaaaa a todxs </b>, este es un texto de prueba :) a continuaci칩n les mostrar칠 un poema de Roberto Bola침o llamado \"Los perros rom치nticos\" 游뱘游游땐\n",
    "\n",
    "https://www.poesi.as/rb9301.htm\n",
    "\n",
    "춰Me gustan los ping칲inos! S칤, los PING칖INOS 游냖游냖游냖 游냖 #VivanLosPinguinos #S칤Se침or #PinguinosDelMundoUn칤os #칌anduesDelMundoTambi칠n\n",
    "\n",
    "Si colaboras con este repositorio te puedes ganar $100.000 (en dinero falso). O tal vez 20 ping칲inos. Mi tel칠fono es +561212121212\"\"\"\n",
    "\n",
    "print(sp.transform(test_text, debug=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a aplicar este preprocesamiento a los textos y guardarlos en una nueva columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado[\"texto_pp\"] = df_filtrado[\"texto\"].apply(lambda x: sp.transform(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploraci칩n a nivel de palabras\n",
    "\n",
    "Ahora, vamos a explorar el corpus a nivel de palabras. Para ello, vamos a crear una funci칩n que reciba un texto y devuelva una lista con las palabras tokenizadas, en min칰sculas, lematizadas, sin `stopwords` y sin n칰meros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizar(text):\n",
    "    doc = nlp(text)\n",
    "    return [\n",
    "        token.lemma_ for token in doc if not token.is_punct and not token.is_stop and token.is_alpha\n",
    "    ]\n",
    "\n",
    "\n",
    "df_filtrado[\"tokens\"] = df_filtrado[\"texto_pp\"].apply(tokenizar)\n",
    "df_filtrado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar la funci칩n `Counter` de la librer칤a `collections` para contar la frecuencia de las palabras en el corpus. Luego, vamos a mostrar las 30 palabras m치s frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Unir todos los tokens en una lista\n",
    "all_tokens = [token for tokens in df_filtrado[\"tokens\"] for token in tokens]\n",
    "\n",
    "# Contar la frecuencia de las palabras\n",
    "word_freq = Counter(all_tokens)\n",
    "\n",
    "# Obtener las 20 palabras m치s frecuentes\n",
    "top_words = word_freq.most_common(30)\n",
    "\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gramas m치s comunes\n",
    "\n",
    "Los n-gramas son secuencias de n palabras. Por ejemplo, los bi-gramas de la oraci칩n `El perro juega con su pelota` son:\n",
    "- `El perro`\n",
    "- `perro juega`\n",
    "- `juega con`\n",
    "- `con su`\n",
    "- `su pelota`\n",
    "\n",
    "Los n-gramas son 칰tiles para detectar frases o expresiones que se repiten en el corpus. Para ello, vamos a utilizar `CountVectorizer` de `scikit-learn`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "stopwords = list(spacy.lang.es.stop_words.STOP_WORDS)\n",
    "\n",
    "# Crear instancia de CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, vamos a utilizar el m칠todo `fit_transform` de la instancia de `CountVectorizer` para crear una matriz de conteo de n-gramas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear matriz de conteo de n-gramas\n",
    "ngram_matrix = vectorizer.fit_transform(df_filtrado[\"texto_pp\"])\n",
    "\n",
    "ngram_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, vamos a sumar las frecuencias de los n-gramas y a mostrar los n-gramas m치s comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sumar las frecuencias de los n-gramas\n",
    "ngram_freq = np.sum(ngram_matrix, axis=0)\n",
    "\n",
    "# Obtener los n-gramas m치s comunes\n",
    "top_ngrams = [(ngram, ngram_freq[0, index]) for ngram, index in vectorizer.vocabulary_.items()]\n",
    "top_ngrams = sorted(top_ngrams, key=lambda x: x[1], reverse=True)[:30]\n",
    "\n",
    "print(top_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculemos ahora las palabras y los bigramas m치s frecuentes por medio. Para ello concatenaremos todos los textos de cada medio y luego calcularemos sus n_gramas m치s repetidas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medios = df_filtrado[\"medio\"].unique().tolist()\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words=stopwords)\n",
    "\n",
    "# Iterar sobre los medios\n",
    "for medio in medios:\n",
    "    # Filtrar los textos por medio\n",
    "    textos_year = df_filtrado[df_filtrado[\"medio\"] == medio][\"texto_pp\"]\n",
    "\n",
    "    # Unir todos los textos en un solo string\n",
    "    texto_completo = \" \".join(textos_year)\n",
    "\n",
    "    # Crear la matriz de n-gramas\n",
    "    ngram_matrix_year = vectorizer.fit_transform([texto_completo])\n",
    "\n",
    "    # Sumar las frecuencias de los n-gramas\n",
    "    ngram_freq_year = np.sum(ngram_matrix_year, axis=0)\n",
    "\n",
    "    # Obtener los n-gramas y sus frecuencias\n",
    "    top_10_ngrams_year = [\n",
    "        (ngram, ngram_freq_year[0, index]) for ngram, index in vectorizer.vocabulary_.items()\n",
    "    ]\n",
    "\n",
    "    # Ordenar por frecuencia y tomar los 10 m치s frecuentes\n",
    "    top_10_ngrams_year = sorted(top_10_ngrams_year, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    # Imprimir los resultados para el medio actual\n",
    "    print(f\"Medio: {medio}\")\n",
    "    print(top_10_ngrams_year)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizaci칩n de textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder hacer an치lisis m치s profundos de los textos, necesitamos representarlos en forma num칠rica. Este proceso se conoce como `vectorizaci칩n`, es decir, transformar los textos en vectores (secuencias de n칰meros).\n",
    "\n",
    "Existen diversas formas de vectorizar textos. En esta clase vamos a ver dos de ellas: `bag-of-words` y `TF-IDF`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "\n",
    "Se conoce como `bag-of-words` a la representaci칩n de un texto como un vector que contiene la frecuencia de cada palabra en el texto. Es decir, se crea un vector con todas las palabras del corpus y se cuenta cu치ntas veces aparece cada palabra en cada texto.\n",
    "\n",
    "![Bag of ords](https://vitalflux.com/wp-content/uploads/2021/08/Bag-of-words-technique-to-convert-to-numerical-feature-vector-png.png)\n",
    "\n",
    "Tambi칠n, podemos contar la frecuencia de los n-gramas en cada texto. Para ello, nuevamente, vamos a utilizar `CountVectorizer` de `scikit-learn`. Esta vez con el par치metro `ngram_range` (1, 1) (por defecto) para contar palabras y con el par치metro `ngram_range` (1, 2) para contar palabras y 2-gramas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_by_word = CountVectorizer(ngram_range=(1, 1), stop_words=stopwords)\n",
    "\n",
    "# Entrenamos el vectorizador\n",
    "vectorizer_by_word.fit(df_filtrado[\"texto_pp\"])\n",
    "\n",
    "# Lo aplicamos al texto de ejemplo\n",
    "vectorizer_by_word.transform([texto_prueba])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado del texto es un vector muy  largo, que corresponde a la cantidad de palabras 칰nicas en el corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a vectorizar un texto con `ngram_range` (1, 2) para contar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_by_2gram = CountVectorizer(ngram_range=(1, 2), stop_words=stopwords)\n",
    "\n",
    "# Entrenamos el vectorizador\n",
    "vectorizer_by_2gram.fit(df_filtrado[\"texto_pp\"])\n",
    "\n",
    "# Lo aplicamos al texto de ejemplo\n",
    "vectorizer_by_2gram.transform([texto_prueba])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos una matriz a칰n m치s grande que la anterior. Esto se debe a que ahora estamos contando palabras y 2-gramas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una dimensionalidad muy alta puede ser un problema, debido a que es m치s costoso computacionalmente de procesar. Por lo tanto, vamos a limitar la cantidad de palabras que se van a considerar en la vectorizaci칩n. Para ello, vamos a utilizar el par치metro `max_features` de `CountVectorizer` para limitar la cantidad de palabras y n-gramas a los 1000 m치s frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_by_2gram = CountVectorizer(ngram_range=(1, 2), max_features=1000, stop_words=stopwords)\n",
    "\n",
    "# Entrenamos el vectorizador\n",
    "vectorizer_by_2gram.fit(df_filtrado[\"texto_pp\"])\n",
    "\n",
    "# Lo aplicamos al texto de ejemplo\n",
    "vectorizer_by_2gram.transform([texto_prueba])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora aplicamos la vectorizaci칩n a todos los textos del corpus y creamos una matriz de 1000 columnas y tantas filas como textos haya en el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_vectorizado_bow = vectorizer_by_2gram.transform(df_filtrado[\"texto_pp\"])\n",
    "\n",
    "# Lo convertimos a un DataFrame con los nombres de las columnas\n",
    "df_corpus_vectorizado_bow = pd.DataFrame(\n",
    "    corpus_vectorizado_bow.toarray(), columns=vectorizer_by_2gram.get_feature_names_out()\n",
    ")\n",
    "df_corpus_vectorizado_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque puede parecer una vectorizaci칩n sencilla y poco sofisticada, `bag-of-words` es una t칠cnica bastante 칰til ya que logra capturar el significado de los textos. Por ejemplo, si tenemos dos textos que hablan de f칰tbol, es probable que compartan palabras como `gol`, `equipo`, `jugador`, `partido`, etc. Por lo tanto, es probable que los vectores de estos textos sean similares. Por ejemplo, podemos buscar el texto que m치s se parece a `texto_prueba`. \n",
    "\n",
    "Para ello recorremos todos los textos del corpus y calculamos la similitud `coseno` con el texto de prueba. La similitud coseno es una medida de similitud entre dos vectores que mide el coseno del 치ngulo entre ellos. Si los vectores son id칠nticos, la similitud coseno es 1. Si los vectores son ortogonales, la similitud coseno es 0. Si los vectores son opuestos, la similitud coseno es -1.\n",
    "\n",
    "Para calcular la similitud coseno, vamos a utilizar la funci칩n `cosine_similarity` de `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Hay que calcular la similitud entre el texto de prueba y todos los dem치s.\n",
    "matriz_similitud = cosine_similarity(corpus_vectorizado_bow[100], corpus_vectorizado_bow)\n",
    "\n",
    "# Ordenar los textos por similitud\n",
    "textos_similares = list(zip(df_filtrado[\"texto_pp\"], matriz_similitud[0]))\n",
    "textos_similares.sort(key=lambda x: -x[1])\n",
    "\n",
    "# Mostrar los 5 textos m치s similares\n",
    "for texto, similitud in textos_similares[:6]:\n",
    "    print(f\"Similitud: {similitud}\\n{texto}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Aunque `bag-of-words` puede ser 칰til, no logra capturar la importancia de las palabras, ya que solo cuenta su frecuencia. Para ello, se puede usar `TF-IDF` (Term Frequency - Inverse Doucument Frequency), que cuenta la frecuencia de las palabras ponderada por su frecuencia inversa en los textos. Es decir, si una palabra aparece en muchos textos, no aporta mucha informaci칩n para distinguir entre textos, por lo tanto se le asigna una ponderaci칩n menor.\n",
    "\n",
    "Para aplicar `TF-IDF` sobre el corpus, vamos a utilizar la funci칩n `TfidfVectorizer` de `scikit-learn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Creamos la instancia de TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=1000, stop_words=stopwords)\n",
    "\n",
    "# Entrenamos el vectorizador y transformamos el texto\n",
    "# (lo hacemos en un solo paso con fit_transform)\n",
    "corpus_vectorizado_tfidf = vectorizer.fit_transform(df_filtrado[\"texto_pp\"])\n",
    "\n",
    "# convertir a DataFrame\n",
    "df_corpus_vectorizado_tfidf = pd.DataFrame(\n",
    "    corpus_vectorizado_tfidf.toarray(), columns=vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# Calculamos la similitud del texto de prueba con los dem치s\n",
    "matriz_similitud_tfidf = cosine_similarity(corpus_vectorizado_tfidf[100], corpus_vectorizado_tfidf)\n",
    "\n",
    "# Ordenamos los textos por similitud\n",
    "textos_similares_tfidf = list(zip(df_filtrado[\"texto_pp\"], matriz_similitud_tfidf[0]))\n",
    "textos_similares_tfidf.sort(key=lambda x: -x[1])\n",
    "\n",
    "# Mostramos los 5 textos m치s similares\n",
    "for texto, similitud in textos_similares_tfidf[:5]:\n",
    "    print(f\"Similitud: {similitud}\\n{texto}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraer t칩picos de los textos\n",
    "\n",
    "Cuando tenemos muchos textos, es 칰til poder extraer los t칩picos de los textos, es decir, las tem치ticas que abordan los textos. Para ello, vamos a utilizar dos m칠todos.\n",
    "\n",
    "Para ello ocuparemos `Latent Dirichlet Allocation` (LDA), que es un modelo generativo de t칩picos. Este modelo asume que cada texto se compone de una mezcla de t칩picos y que cada t칩pico se compone de una mezcla de palabras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA con TF-IDF\n",
    "\n",
    "Vamos a aplicar LDA a la matriz generada por `tf-idf`. Si bien tambi칠n podr칤amos ocupar Bag of Words, TF-IDF es m치s adecuado para LDA, ya que pondera las palabras por su importancia en los textos.\n",
    "\n",
    "Creamos la instancia de LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=0)\n",
    "\n",
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos LDA\n",
    "lda.fit(corpus_vectorizado_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos obtener los t칩picos con el m칠todo `components_` de la instancia de LDA:\n",
    "\n",
    "topicos = lda.components_\n",
    "topicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos una matriz de 10 t칩picos y 1000 palabras.\n",
    "\n",
    "Vamos a mostrar las 20 palabras que componen cada t칩pico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las palabras de cada t칩pico\n",
    "words = vectorizer_by_2gram.get_feature_names_out()\n",
    "\n",
    "top_words_per_topic = []\n",
    "\n",
    "for numero_topico in range(10):\n",
    "    print(f\"\\nT칩pico {numero_topico + 1}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Obtener los pesos de las palabras para este t칩pico\n",
    "    pesos_palabras = topicos[numero_topico]\n",
    "\n",
    "    # Crear lista de tuplas (palabra, peso)\n",
    "    palabras_y_pesos = list(zip(words, pesos_palabras))\n",
    "\n",
    "    # Ordenar por peso de mayor a menor\n",
    "    palabras_y_pesos = sorted(palabras_y_pesos, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Mostrar las 20 primeras palabras\n",
    "    for palabra, peso in palabras_y_pesos[:10]:\n",
    "        print(f\"Palabra: {palabra:<20} Peso: {peso:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracci칩n de T칩picos con BERTopic\n",
    "\n",
    "LDA (Latent Dirichlet Allocation) es un m칠todo popular para la extracci칩n de t칩picos, pero presenta algunas limitaciones:\n",
    "\n",
    "- LDA supone que los documentos se generan a partir de una mezcla de t칩picos y que cada t칩pico es una mezcla de palabras, lo cual puede no ser cierto en todos los casos.\n",
    "- LDA no siempre maneja bien documentos cortos o con t칩picos poco definidos.\n",
    "\n",
    "### BERTopic: un enfoque m치s moderno\n",
    "\n",
    "BERTopic utiliza modelos de lenguaje basados en `Transformers` (como BERT) para generar representaciones vectoriales de textos (`embeddings`). Luego, agrupa estos vectores para identificar t칩picos, lo que ofrece ventajas sobre LDA:\n",
    "- No asume una distribuci칩n espec칤fica de palabras en los t칩picos.\n",
    "- Capta mejor el contexto sem치ntico de los textos.\n",
    "- Es m치s robusto con textos cortos\n",
    "\n",
    "\n",
    "### Instalaci칩n de BERTopic\n",
    "\n",
    "Para comenzar, instalamos la librer칤a con el siguiente comando:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bertopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos la librer칤a y creamos una instancia de BERTopic. Utilizaremos el modelo multiling칲e `paraphrase-multilingual-MiniLM-L12-v2`, que soporta espa침ol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# Crear una instancia del modelo BERTopic\n",
    "topic_model = BERTopic(\n",
    "    language=\"multilingual\", embedding_model=\"paraphrase-multilingual-MiniLM-L12-v2\", verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo con los textos originales. No es necesario preprocesar los textos, ya que los modelos de lenguaje basados en Transformers manejan bien los textos en bruto.\n",
    "\n",
    "El resultado ser치 un DataFrame que incluye:\n",
    "- ID del t칩pico: Identificador 칰nico.\n",
    "- Frecuencia: N칰mero de textos asignados al t칩pico.\n",
    "- Palabras representativas: T칠rminos m치s importantes del t칩pico.\n",
    "- T칩pico -1: Corresponde a los textos que no pudieron asignarse a ning칰n t칩pico (outliers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo y obtener los t칩picos\n",
    "topics, probs = topic_model.fit_transform(df_filtrado[\"texto\"])\n",
    "\n",
    "# Obtener informaci칩n detallada de los t칩picos\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignar t칩picos a los textos\n",
    "df_filtrado[\"topico_bertopic\"] = topics\n",
    "df_filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el t칩pico m치s frecuente por medio\n",
    "\n",
    "\n",
    "for medio in sorted(df_filtrado[\"medio\"].unique()):\n",
    "    # Filter comments for this year\n",
    "    noticias_medio = df_filtrado[df_filtrado[\"medio\"] == medio]\n",
    "\n",
    "    # Filter comments with no topic (-1)\n",
    "    noticias_medio = noticias_medio[noticias_medio[\"topico_bertopic\"] != -1]\n",
    "\n",
    "    # Count frequency of each topic\n",
    "    frecuencia_topicos = noticias_medio[\"topico_bertopic\"].value_counts()\n",
    "\n",
    "    # Get most frequent topic\n",
    "    topico_mas_frecuente = frecuencia_topicos.idxmax()\n",
    "\n",
    "    # Get topic info for the most frequent topic\n",
    "    info_topico = topic_model.get_topic(topico_mas_frecuente)\n",
    "\n",
    "    print(f\"\\Medio: {medio}\")\n",
    "    print(f\"T칩pico m치s frecuente: {topico_mas_frecuente}\")\n",
    "    print(f\"Frecuencia: {frecuencia_topicos[topico_mas_frecuente]}\")\n",
    "    print(\"Palabras m치s representativas:\")\n",
    "    for palabra, peso in info_topico[:10]:\n",
    "        print(f\"- {palabra}: {peso:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An치lisis ling칲칤sticos\n",
    "\n",
    "A continuaci칩n vamos a etiquetar textos mediante an치lisis ling칲칤sticos. En concreto, vamos a realizar las siguientes tareas:\n",
    "\n",
    "* An치lisis de categor칤as gram치ticales (POS tagging)\n",
    "* An치lisis de entidades nombradas (NER)\n",
    "\n",
    "En concreto, vamos a utilizar Spacy para realizar el etiquetado de tokens.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para etiquetar un texto con categor칤as gram치ticas, podemos usar el atributo `pos_` de cada token. Este atributo etiqueta las palabras con categor칤as universales, como Sustantivo, Verbo, Adjetivo, etc.\n",
    "texto_prueba = df_filtrado.iloc[13][\"texto\"]\n",
    "print(texto_prueba)\n",
    "pos_texto = [(token.text, token.pos_) for token in nlp(texto_prueba)]\n",
    "pos_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para identificar las entidades nombradas, podemos usar el atributo `ents` del resultado de procesar un texto con `nlp()`. Este atributo retorna una lista de entidades, cada una con su tipo (como Persona, Lugar, Organizaci칩n, etc) y el texto de la entidad.\n",
    "\n",
    "texto_prueba = df_filtrado.iloc[20][\"texto\"]\n",
    "print(texto_prueba)\n",
    "entidades = [(ent.text, ent.label_) for ent in nlp(texto_prueba).ents]\n",
    "entidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificaci칩n de textos\n",
    "\n",
    "En muchas ocasiones, es necesario clasificar textos en distintas categor칤as. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos preentrenados con HuggingFace\n",
    "\n",
    "HuggingFace es una librer칤a que permite acceder a modelos preentrenados de lenguaje natural. En particular, vamos a utilizar modelos preentrenados para clasificar textos en las siguientes categor칤as:\n",
    "\n",
    "- An치lisis de sentimientos\n",
    "- Hate speech\n",
    "\n",
    "Puedes encontrar m치s informaci칩n sobre los modelos preentrenados en espa침ol de HuggingFace en [este enlace](https://huggingface.co/models?pipeline_tag=text-classification&language=es&sort=trending)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero vamos a instalar la librer칤a `transformers` de HuggingFace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a utilizar el modelo `pysentimiento/robertuito-sentiment-analysis` para clasificar los textos dependiendo de su sentimiento. Este modelo clasifica los textos en tres categor칤as: `positivo`, `negativo` y `neutro`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Crea una instancia del pipeline de an치lisis de sentimiento\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"pysentimiento/robertuito-sentiment-analysis\",\n",
    "    return_all_scores=True,\n",
    ")\n",
    "\n",
    "# Texto de prueba\n",
    "texto_prueba = \"la constituci칩n es un mamarracho\"\n",
    "\n",
    "sentiment_analyzer(texto_prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a utilizar el modelo `pysentimiento/robertuito-hate-speech` para clasificar los textos dependiendo de si contienen discurso de odio o no. Este modelo clasifica los textos en las siguientes clases:\n",
    "- HS: si el texto contiene discurso de odio.\n",
    "- TR: si el texto est치 dirigido a una persona espec칤fica.\n",
    "- AG: si el texto es agresivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una instancia del pipeline de an치lisis de hate\n",
    "hate_analyzer = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"pysentimiento/robertuito-hate-speech\",\n",
    "    return_all_scores=True,\n",
    ")\n",
    "\n",
    "# Texto de prueba\n",
    "texto_prueba = \"los constituyentes son unos estupidos\"\n",
    "\n",
    "hate_analyzer(texto_prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procesemos ahora todo el dataset con el modelo de an치lisis de sentimientos y de hate speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filtrado[\"sentimiento\"] = df_filtrado[\"texto\"].apply(lambda x: sentiment_analyzer(x))\n",
    "# df_filtrado[\"hate_speech\"] = df_filtrado[\"texto\"].apply(lambda x: hate_analyzer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An치lisis de textos con IA generativa\n",
    "\n",
    "La IA generativa ha ganado popularidad en los 칰ltimos a침os para el an치lisis de textos ya que no requiere de datos preetiquetados. Basta con introducir las instrucciones necesarias para que el modelo genere clasificaciones, an치lisis, res칰menes, traducciones o lo que se necesite.\n",
    "\n",
    "Existen m칰ltiples modelos de IA generativa, como ChatGPT, Gemini, Llama, Claude, entre otros. \n",
    "\n",
    "En esta clase vamos a utilizar Langchain, un paquete que nos permite trabajar con IA generativa de forma agn칩stica a los modelos. \n",
    "\n",
    "A continuaci칩n, ocuparemos Gemini, la IA generativa de Google, para detectar las noticias que contienen sesgos. \n",
    "\n",
    "Primero, vamos a instalar las dependencias necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ocupar los modelos de IA generativa necesitas una API Key del proveedor. En este caso, usaremos la API de ChatGPT.\n",
    "\n",
    "Para obtener la API Key, debes registrarte en OpenAI, ingresar tu tarjeta de cr칠dito y generar la API Key.\n",
    "    \n",
    "https://platform.openai.com/docs/overview\n",
    "\n",
    "Considera que el uso de estos servicios tiene un costo asociado. Puedes ver el detalle de los precios en la siguiente p치gina:\n",
    "    \n",
    "https://openai.com/api/pricing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = (\n",
    "    \"xxx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,  # Grado de creatividad. 0 significa que es poco creativo y 1 es muy creativo\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "llm.invoke(\"Saludame en mapudungun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "tagging_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Analiza la siguiente noticia y determina si el contenido o la forma de estar escrito puede generar temor en el lector.\n",
    "\n",
    "Entenderemos como temor la sensaci칩n de miedo, angustia o preocupaci칩n que puede generar un texto en el lector.\n",
    "\n",
    "Solo extrae las propiedades mencionadas en la funci칩n 'Classification'.\n",
    "\n",
    "Texto:\n",
    "{texto}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class Classification(BaseModel):\n",
    "    razonamiento: str = Field(description=\"El razonamiento detr치s de la clasificaci칩n\")\n",
    "    temor: bool = Field(description=\"Si el texto genera o no temor\")\n",
    "\n",
    "\n",
    "chain = tagging_prompt | llm.with_structured_output(Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticia = df[\"texto\"].iloc[2]\n",
    "\n",
    "resultado = chain.invoke({\"texto\": noticia})\n",
    "print(\"Noticia:\")\n",
    "print(noticia)\n",
    "print(\"\\n\\nRazonamiento:\")\n",
    "print(resultado.razonamiento)\n",
    "print(\"\\n\\nGenera temor:\", resultado.temor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_filtrado_sample = df_filtrado.sample(200, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_chain(texto):\n",
    "    resultado = chain.invoke({\"texto\": texto})\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Crea 3 nuevas columnas en el DataFrame con los resultados de la clasificaci칩n\n",
    "# df_filtrado[\"clasificacion_estereotipo\"] = df_filtrado[\"texto\"].apply(aplicar_chain)\n",
    "\n",
    "# df_filtrado[\"razonamiento\"] = df_filtrado[\"clasificacion_estereotipo\"].apply(\n",
    "#     lambda x: x.razonamiento\n",
    "# )\n",
    "# df_filtrado[\"genera_temor\"] = df_filtrado[\"clasificacion_estereotipo\"].apply(lambda x: x.temor)\n",
    "# df_filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filtrado.to_csv(\"noticias_procesadas.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para no esperar tanto tiempo, descargamos el archivo ya procesado\n",
    "!wget https://github.com/jorgeortizfuentes/sicss-text-analysis2025/raw/refs/heads/main/noticias_procesadas.tsv\n",
    "\n",
    "df_filtrado = pd.read_csv(\"noticias_procesadas.tsv\", sep=\"\\t\")\n",
    "\n",
    "df_filtrado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando un clasificador\n",
    "\n",
    "El uso de IA generativa es muy 칰til para el an치lisis de textos. Sin embargo, es caro y no genera resultados consistentes. \n",
    "\n",
    "El enfoque cl치sico para entrenar un clasificador de texto requiere de datos etiquetados. Por lo tanto, podemos aprovechar la IA generativa para generar datos etiquetados y luego entrenar un clasificador de texto.\n",
    "\n",
    "En esta ocasi칩n utilizaremos los embeddings de 췂paraphrase-multilingual-MiniLM-L12-v2 para convertir los textos en vectores y luego entrenaremos un modelo utilizando scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentences = [\"Esto es un texto de ejemplo\"]\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a leer el archivo guardado\n",
    "df_filtrado = pd.read_csv(\"noticias_procesadas.tsv\", sep=\"\\t\")\n",
    "df_filtrado[\"genera_temor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizar el df_filtrado con el modelo SentenceTransformer\n",
    "\n",
    "df_filtrado[\"embeddings\"] = df_filtrado[\"texto\"].apply(lambda x: model.encode([x])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo de random forest para clasificar los textos (con genera_temor y embeddings)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rfc_model = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Dividir dataset\n",
    "X = df_filtrado[\"embeddings\"].tolist()\n",
    "y = df_filtrado[\"genera_temor\"].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Entrenar modelo\n",
    "rfc_model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir\n",
    "y_pred = rfc_model.predict(X_test)\n",
    "\n",
    "# Mostrar reporte de clasificaci칩n\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos ahora con xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=0)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Material complementario \n",
    "\n",
    "* [Speech and Language Processing (Jurafsky and Martin)](https://web.stanford.edu/~jurafsky/slp3/)\n",
    "* [spaCy 101](https://spacy.io/usage/spacy-101)\n",
    "* [LangChain Tutorials](https://python.langchain.com/docs/tutorials/)\n",
    "* [Spanish NLP library](https://github.com/jorgeortizfuentes/spanish_nlp) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
